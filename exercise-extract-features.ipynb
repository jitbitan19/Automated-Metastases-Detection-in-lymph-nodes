{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extracting Features from Heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Requirements](#Requirements) \n",
    "  * [Modules](#Python-Modules)\n",
    "  * [Data](#Data)\n",
    "* [Teaching Content](#Teaching-Content)\n",
    "  * [Load and View Heatmaps](#Load-and-View-Heatmaps)\n",
    "* [Exercises](#Exercises)\n",
    "* [Summary and Outlook](#Summary-and-Outlook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is the first of three notebooks about the CAMELYON17 challenge. In this challenge, the task is to predict a patients pN-stage (pN0, pn0(i+), pNmi, pN1 and pN2). The pN-stage is determined by the lables of the five slides corresponding to the patient. Once the lables of the individual slides are known (_negative, itc, micro, macro_), the patient's pN-stage can be looked up in a table (see the official [CAMELYON17 website](https://camelyon17.grand-challenge.org/Evaluation/)). So the real difficulty of the challenge is to predict the labels of the slides.\n",
    "\n",
    "Unfortunately the training data here is very limited as we have only 500 slides from the CAMELYON17 training set and 130 labled slides from the CAMELYON16 test set (labled with _negative, itc, micro, macro_). So opposed to the task of the CAMELYON16 challenged where we had thousands of tiles and only two different labels (_normal_ and _tumor_), we will not be able to supply another CNN model with sufficient data. Even worse, for the _itc_ class, we only have 35 examples in total.\n",
    "\n",
    "To tackle this problem, our approach is to make use of domain specific knowledge and extract geoemtrical features from the heatmaps, which can be used to train a less complex model, e.g, a decision tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Python-Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# External Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import skimage\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import ndimage\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "In this notebook you will be provided with heatmaps of slides of the CAMELYON16 test set and the CAMELYON17 set. Using them has some advantages over the heatmaps you might have created in the last notebook:\n",
    "- You will not have to download the CAMELYON17 data set (~2.7 tera bytes).\n",
    "- The heatmaps you will be provided with are a lot more accurate:\n",
    "  - They were created with a far superior model.\n",
    "  - The model did not only use zoom level 2. Instead two CNNs were trained. One on level 0 and one on level 1.\n",
    "  - As a consequence of the higher zoom, they also have a higher resolution.\n",
    "  - The CNNs' architecture was a more sophisticated one, the Inception v4 architecture (for details see [[SZE17](https://arxiv.org/abs/1602.07261)]).\n",
    "  - Algorithm for advanced (domain specific) color normalization was applied [[BEJ16]].\n",
    "  - All kinds of data augmentation were used.\n",
    "  - The CNNs were trained on two Geforce Titan X GPUs for over 2 weeks.\n",
    "  - You can read more about the techniques used in [[STR18](https://camelyon17.grand-challenge.org/media/evaluation-supplementary/80/6390/8875d833-cd31-4b61-b58e-04c6b33dc039/strohmengerKlaus_h_9Ehs19x.pdf)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "**Short:**\n",
    "\n",
    "The heatmaps of the sldies were created with a CNN. Each heatmap represents a Whole-Slide-Image (WSI) of tissue. A WSI is approximately 200.000 x 100.000 pixels big. Each slide was cut into 256 x 256 pixel tiles. The CNN was trained to predict these tiles whether they contain tumorous tissue or not. One pixel of the provided heatmaps represents the prediction of the CNN of one 256 x 256 tile. \n",
    "\n",
    "**Long:**\n",
    "\n",
    "For more information have a look at the [deepTEACHING website](https://www.deep-teaching.org/medical-image-classification), or the official homepage of the [CAMEYLON Challenge](https://camelyon17.grand-challenge.org/Background/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "1. Download the heatmaps from **[here](https://gitlab.com/deep.TEACHING/educational-materials/blob/master/notebooks/data/c16traintest_c17traintest_heatmaps_grey.zip)** and unpack them.\n",
    "2. Adjust the path variables:\n",
    " - `HEATMAP_DIR`\n",
    " - `PATH_C16_LABELS`\n",
    " - `PATH_C17_LABELS`\n",
    " - `HEATMAP_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### EDIT THIS CELL:\n",
    "### Assign the path to your CAMELYON16 data and create the directories\n",
    "### if they do not exist yet.\n",
    "CAM_BASE_DIR = '/path/to/CAMELYON/data/'\n",
    "\n",
    "# exmple:\n",
    "CAM_BASE_DIR = '/media/klaus/2612FE3171F55111/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATED_DATA = CAM_BASE_DIR + 'tutorial/' \n",
    "HEATMAP_DIR = CAM_BASE_DIR + 'c16traintest_c17traintest_heatmaps_grey/'\n",
    "\n",
    "# CAMELYON16 and 17 ground truth labels\n",
    "PATH_C16_LABELS = CAM_BASE_DIR + 'CAMELYON16/test/reference.csv'\n",
    "PATH_C17_LABELS = CAM_BASE_DIR + 'CAMELYON17/training/stage_labels.csv'\n",
    "\n",
    "# Just one test slide for testing\n",
    "HEATMAP_Test_001 = HEATMAP_DIR + 'Test_001.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teaching Content\n",
    "\n",
    "### Load and View Heatmaps\n",
    "\n",
    "To get a first impression of the heatmaps, we will first load and view one. A pixels value describes the _\"degree of believe\"_ of the CNNs, the region contains tumorous tissue (1.0) or not (0.0).\n",
    "\n",
    "Executing the next two cells should show the following images:\n",
    "\n",
    "![internet connection needed](https://gitlab.com/deep.TEACHING/educational-materials/raw/master/media/klaus/medical-image-classification/Test_001_grey_heatmap.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 'Test_001.png'\n",
    "example_heatmap_grey = skimage.io.imread(HEATMAP_Test_001, as_gray=True)\n",
    "\n",
    "# View the dimensions\n",
    "print(example_heatmap_grey.shape)\n",
    "\n",
    "# View min / max values\n",
    "print(example_heatmap_grey.min())\n",
    "print(example_heatmap_grey.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View as greyscale\n",
    "\n",
    "axs = plt.subplot()\n",
    "plt.imshow(example_heatmap_grey,cmap=plt.get_cmap('Greys_r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View as heatmap (just for visualization purpose)\n",
    "\n",
    "plt.imshow(example_heatmap_grey,cmap=plt.get_cmap('jet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In total we will extract 6 features of every heatmap:\n",
    "\n",
    "1. Highest probability (value) on the heatmap (_red_)\n",
    "2. Average probability on the heatmp. Sum all values and divide by the number of values $\\gt 0.0$ (_green_)\n",
    "3. Number of pixels after thresholding (_pink_)\n",
    "4. Length of the larger side of the biggest object after thresholding (_orange_)\n",
    "5. Length of the smaller side of the biggest object after thresholding (_yellow_)\n",
    "6. Number of pixels of the biggest object after thresholding (_blue_)\n",
    "\n",
    "![internet connection needed](https://gitlab.com/deep.TEACHING/educational-materials/raw/master/media/klaus/medical-image-classification/features.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['highest_probability',\n",
    "                 'average_porbability',\n",
    "                 'sum_pixel_after_threshold',\n",
    "                 'biggest_object_sum_pixels',\n",
    "                 'biggest_object_large_side',\n",
    "                 'biggest_object_small_side']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract First Two Features\n",
    "\n",
    "The first to features can easily be extracted without further processing:\n",
    "- highest probability\n",
    "- average probability (not of all pixels, only pixels $\\gt 0.0$)\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the functions to extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise\n",
    "\n",
    "def extract_feature_highest_probability(greyscale_img):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def extract_feature_avg_probability(greyscale_img):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = extract_feature_highest_probability(example_heatmap_grey)\n",
    "f2 = extract_feature_avg_probability(example_heatmap_grey)\n",
    "print(feature_names[0], f1)\n",
    "print(feature_names[1], f2)\n",
    "\n",
    "\n",
    "### NOTE: These tests can only pass when using the provided heatmaps!\n",
    "### Comment these lines out if you use your own heatmaps\n",
    "assert 1.0 > f1 > 0.97\n",
    "assert 0.24 > f2 > 0.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize Greyscale Heatmaps\n",
    "\n",
    "In order to extract the next four features, we must clearly define which value between 0.0 and 1.0 (and above) we believe is tumorous tissue. \n",
    "\n",
    "A logical value would be 0.5, since that is the value the CNN used as decision boundary when it was trained to predict if a tile of 256x256 pixels contains tumor or not.\n",
    "\n",
    "But lower or higher values might be beneficial as well. Lower values will yield bigger regions of connected pixel. Higher values will contain less noise.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the function to transform a greyscale image into a binary image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Apply global threshold (e.g. 0.5)\n",
    "\n",
    "def get_binary_of_greyscale_heatmap(greyscale_img, threshold=0.5):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_binary = get_binary_of_greyscale_heatmap(example_heatmap_grey, 0.5)\n",
    "plt.imshow(heatmap_binary,cmap=plt.get_cmap('Greys_r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Third Feature\n",
    "\n",
    "Now that we have a binarized heatmap we can extract the feature\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the function to extract the feature  *sum_pixel_after_threshold*. In other words, the area that is left.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_sum_after_binarization(binary_image):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = extract_feature_sum_after_binarization(heatmap_binary)\n",
    "print(feature_names[2], f3)\n",
    "\n",
    "\n",
    "### NOTE: These tests can only pass when using the provided heatmaps!\n",
    "### Comment these lines out if you use your own heatmaps\n",
    "assert f3 == 1865 ### only for threshold 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Biggest Object\n",
    "\n",
    "Now the last three feature can be a bit tricky. First we need to find the biggest object in the image. The biggest object means, the largest area of connected pixel. A pixel is connected to another if it is the very next pixel to the left / right / up / down.\n",
    "\n",
    "After we identified the individual objects (areas of connected pixels), we need to extract their features:\n",
    "- area of the connected region (= biggest object) in sum of the pixels\n",
    "- large side of the connected region\n",
    "- small side of the connected region\n",
    "\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the function to identify objects (connected regions) in a binary image and to extract their features.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "If you cannot manage to extract these three features you can proceed with this notebook and the others following only using the first three features, though classification results at the end of the series of the notebooks will not be as accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_objects_features_in_binary_image(binary_image):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, the output of the cell below could look similar to the following:\n",
    "\n",
    "```\n",
    "object number\t|sum_pixels\t|large_side\t|small_side\n",
    "0\t\t\t\t|1 \t\t\t|1.0\t\t|1.0\n",
    "1\t\t\t\t|1 \t\t\t|1.0\t\t|1.0\n",
    "2\t\t\t\t|1 \t\t\t|1.0\t\t|1.0\n",
    "3\t\t\t\t|3 \t\t\t|2.0\t\t|2.0\n",
    "4\t\t\t\t|1359 \t\t |61.0\t\t|39.0\n",
    "5\t\t\t\t|1 \t\t\t|1.0\t\t|1.0\n",
    "6\t\t\t\t|2 \t\t\t|2.0\t\t|1.0\n",
    "...\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftrs_objects = find_objects_features_in_binary_image(heatmap_binary)\n",
    "\n",
    "print('object number\\t|%s\\t|%s\\t|%s' %('sum_pixels', 'large_side', 'small_side'))\n",
    "print()\n",
    "for i in range(len(ftrs_objects)):\n",
    "    print('%d\\t\\t|%d \\t\\t|%.1f\\t\\t|%.1f' % (i, ftrs_objects[i,0], ftrs_objects[i,1], ftrs_objects[i,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can just sort the objects according to the feature *object_sum_pixels* to get the features of the biggest object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_obj_idx = np.argmax(ftrs_objects[:,0])\n",
    "f4 = ftrs_objects[biggest_obj_idx][0]\n",
    "f5 = ftrs_objects[biggest_obj_idx][1]\n",
    "f6 = ftrs_objects[biggest_obj_idx][2]\n",
    "print('features of biggest object:')\n",
    "print('%s: %.2f' %(feature_names[3], f4))\n",
    "print('%s: %.2f' %(feature_names[4], f5))\n",
    "print('%s: %.2f' %(feature_names[5], f6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features of all Heatmaps\n",
    "\n",
    "Now we have everything we need to extract the six features from the heatmaps.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Extract all six features from all heatmaps and combine them with their labels (_negative_, _itc_, _micro_, _macro_) in a pandas `DataFrame`\n",
    "\n",
    "For the next notebook, it is crucial to add the data in different pandas `DataFrame` objects:\n",
    "- one for CAMEYLON16 test data set (heatmaps named _Test_xxx.png_)\n",
    "- one for CAMELYON17 train data set (named _pantient_0xx.png_)\n",
    "- one for CAMELYON17 test data set (_named _patient_1xx.png_)\n",
    "\n",
    "Executing the three cells below should show output similar to the one in this picture:\n",
    "\n",
    "![internet connection needed](https://gitlab.com/deep.TEACHING/educational-materials/raw/master/media/klaus/medical-image-classification/extracted_features_pandas.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "c16_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c17_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c17_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c16_test.to_csv(GENERATED_DATA + 'features_c16_test.csv')\n",
    "c17_train.to_csv(GENERATED_DATA +'features_c17_train.csv')\n",
    "c17_test.to_csv(GENERATED_DATA +'features_c17_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Outlook\n",
    "\n",
    "Next we will train a simple classifier with the features we just extracted. \n",
    "\n",
    "Possible improvements for this notebook:\n",
    "- Extract more features (e.g. last three features also of the second biggest object)\n",
    "- More advanced measuring of the length of the biggest object (instead of just using a bounding box)\n",
    "- One or more preprocessing steps after binarization to connect near regions or to reduce noise e.g. using:\n",
    " - Morphological image operations (morphological opening / closing)\n",
    " - More advanced: clustering algorithms, e.g. DBSCAN [LEE19]\n",
    "- Extract Features for multiple thesholds and use them all together, e.g. 0.1, 0.2, ..., 0.9 [PIN19][ZHA19]\n",
    "\n",
    "**Note:**\n",
    "\n",
    "If you want to add complete new features, it is always a good advice to know about the domain, so you do not extract arbitrary features, which have nothing to do with the classification decision. Read the corresponding [section](https://camelyon17.grand-challenge.org/Evaluation/) on the CAMELYON website, how real pathologists decide about the label (*negative, itc, micro, macro*) of a slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Literature\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"BEJ16\"></a>[BEJ16]\n",
    "        </td>\n",
    "        <td>\n",
    "            Babak Ehteshami Bejnordi, Geert Litjens, Nadya Timofeeva,Irene Otte-Holler, Andr ¨ e Homeyer, Nico Kars- ´semeijer, and Jeroen AWM van der Laak, “Stain specific standardization of whole-slide histopathological images,” IEEE transactions on medical imaging, vol.35, no. 2, pp. 404–415, 2016.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"LEE19\"></a>[LEE19]\n",
    "        </td>\n",
    "        <td>\n",
    "Lee,   S.,   Oh,   S.,   Choi,   K.,   Kim,   S.:   Automatic   Classification   on   Patient-Level  Breast  Cancer  Metastases.  In:  Submission  results  Camelyon17  challange,https://camelyon17.grand-challenge.org/evaluation/results/. Last accessed 18 Nov2019hole-slide histopathological images,” IEEE transactions on medical imaging, vol.35, no. 2, pp. 404–415, 2016.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"PIN19\"></a>[PIN19]\n",
    "        </td>\n",
    "        <td>\n",
    "Pinchaud,    N.,    Hedlund,    M.:    Camelyon17    Grand    Challenge.    In:    Sub-missionresultsCamelyon17challange,https://camelyon17.grand-challenge.org/evaluation/results/. Last accessed 18 Nov 2019\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"ZHA19\"></a>[ZHA19]\n",
    "        </td>\n",
    "        <td>\n",
    " Zhao, Z., Lin, H., Heng, P.: Breat Cancer pN-Stage Classification for Whole SlideImages. In: Submission results Camelyon17 challange, https://camelyon17.grand-challenge.org/evaluation/results/. Last accessed 18 Nov 2019\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"STR18\"></a>[STR18]\n",
    "        </td>\n",
    "        <td>\n",
    "            Klaus Strohmenger, Jonas Annuscheit, Iris Klempert, Benjamin Voigt, Christian Herta, Peter Hufnagl. \"CONVOLUTIONAL NEURAL NETWORKS AND RANDOM FORESTS FOR DETECTION AND CLASSIFICATION OF METASTASIS IN HISTOLOGICAL SLIDES. https://camelyon17.grand-challenge.org/media/evaluation-supplementary/80/6390/8875d833-cd31-4b61-b58e-04c6b33dc039/strohmengerKlaus_h_9Ehs19x.pdf\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"SZE17\"></a>[SZE17]\n",
    "        </td>\n",
    "        <td>\n",
    "            Szegedy, Christian, et al. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" AAAI. Vol. 4. 2017.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "XXX<br/>\n",
    "by Klaus Strohmenger<br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 Klaus Strohmenger\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "deep_teaching_kernel",
   "language": "python",
   "name": "deep_teaching_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
